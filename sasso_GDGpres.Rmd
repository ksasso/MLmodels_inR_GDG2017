---
title: "Implementing Popular ML Algorithms in R"
author: "Katie Sasso"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
    incremental: true


---
<style>
.column-left{
  float: left;
  width: 46%;
  text-align: left;
}
</style>

<style>
.column-left3{
  float: left;
  width: 32%;
  text-align: left;
}
.column-center3{
  display: inline-block;
  width: 32%;
  text-align: center;
}
.column-right3{
  float: right;
  width: 32%;
  text-align: right;
}
</style>

<style>
pre {
  white-space: pre !important;
  overflow-y: scroll !important;
  height: 50vh !important;
}
</style>

## Materials

https://github.com/ksasso/MLmodels_inR_GDG2017

##Machine Learning Types and Algorithms

<div class="column-left">

**Supervised**:

- The outcome (or dependent variable, DV) is known or observed
- DV is predicted from independent variables (IVs or predictors)
- Training until a desired level of accuracy is reached
- Some examples
    + Regression, Random Forest, KNN (k-nearest-neighbor)
</div>

<div class="column-left">

**Unsupervised**:

- The outcome or DV is not known (no target or outcome to predict)
- Often used for clustering population into different groups
- Examples:
    + Various clustering algorithms (K-means), NLP algorithms (Latent Dirichlet Allocation or LDA)

</div>

## Resampling Methods

<div class="column-left">

- Prediction error
    + The deviation of actual values from the predicted values
    + Calculating this using the same observations used to train the model can lead to underestimating the prediction error
    
- Resampling Methods
    + Refit a model of interest to a second set of observations, or a test set
    + Predicting responses of new observations, that were not used in training the model
    + Way of reducing bias when evaluating model fit

</div>

<div class="column-left">

- Validation-set approach
    + Randomly divide the available sample into a training set and a hold-out set
    + Fit model to training set, use fitted model to predict responses from the test set
    + Test error can be highly variable, depending on which obs are included 
    
- _K_-fold Cross-Validation (CV)
    + Randomly divide the data into _K_ equal-sized parts.
    + Leave out part _k_, fit the model to the other _K_-1 parts (combined) and obtain predictions for the left-out _k_ th part.
    + Each "fold" gets a turn as a training and test set and prediction error is calculated. Combine results to assess overall prediction error

</div>

## Tree-based methods for regression and classification:

- CART (Classification and Regression Trees)
    + Involve stratifying or segmenting the predictor space into a number of simple regions
    + The set of rules used to segment the predictor space can be summarized in a tree 
    + Tree-based methods are simple and useful for interpretation but not as competitive with the best supervised learning approaches in terms of prediction _but can be_ when they are combined (i.e., bagging, random forests, boosting)
    

<div class="notes">

- these methods grow multiple trees which are then combined to yield a single consensus prediction. 
- combining multiple trees can drastically improve accuracy but looses interpretation. End regions are terminal nodes .

</div>  

## Regression Trees: The Basics$^{1}$ 

**Baseball Salary Data: How Would you Stratify it?**

<div class="column-left">

```{r, echo = FALSE, message=FALSE, warning= FALSE, error=FALSE,out.width = "450px"}
knitr::include_graphics("baseball_presice.png")
```

</div>


<div class="column-left">

```{r, echo = FALSE, message=FALSE, warning= FALSE, error=FALSE,out.width = "450px"}
knitr::include_graphics("baseball_cuts.png")
```

</div>


<div class="notes">

greedy - top down (recursive binary splitting). start at the top and do this at each step
minimize this (residual sum of squares or RSS) at EACH STEP(without regard  to any future steps) WITHIN each of the resulting regions 
Process is continued until we reach a stopping point (e.g., no region contains more than 5 obs)

overall tree stratifies or segments the players into 3 **regions of predictor space**

TUNING PARAMETER
Controls the size of the tree to avoid over fitting. Find the best sub tree from growing big tree using cost complexity pruning to the large tree in order to obtain a sequence of best sub trees. Use cross- validation to pick alpha, were alpha is a TURNING PARAMETER that controls a trade-off between the sub tree's complexity and its fit to the training data. After cross validation return to the full data set to obtain the sub tree corresponding to alpha. 

**STOPPING POINT min obs in terminal nodes. FOR CLSASIFICATION - it's just the fraction of the obs in the region that don't belong to most common class**

</div>

***

<div class="column-left">

```{r, echo = FALSE, message=FALSE, warning= FALSE, error=FALSE,out.width = "450px"}
knitr::include_graphics("baseball_tree.png")
```
$^{1}$

</div>

<div class="column-left">

```{r, echo = FALSE, message=FALSE, warning= FALSE, error=FALSE,out.width = "450px", out.height='300px'}
knitr::include_graphics("baseball_tree_ongraph.png")
```
**GOAL**$^{1}$:

Find boxes $R_{1},...,R_{J}$ that minimize the RSS, given by 
\[
\sum_{j = 1}^{J}\sum_{i \in R_{j}}(y_{i}-\hat{y}_{R_{j}})^{2}
\]
where $\hat{y}_{R_{j}}$ is the mean response for the training observation within the _j_ th box

</div>


<div class="notes">

- Overall, the tree stratifies or segments the players into three regions of predictor space: R1 ={X | Years< 4.5}, R2 ={X | Years>=4.5, Hits<117.5}, and R3 ={X | Years>=4.5, Hits>=117.5}.

- At a given internal node, the label (of the form Xj < tk) indicates the left-hand branch emanating from that split, and the right-hand branch corresponds to Xj â‰¥ tk. For instance, the split at the top of the tree results in two large branches. The left-hand branch corresponds to Years<4.5, and the right-hand branch corresponds to Years>=4.5.

- The tree has two internal nodes and three terminal nodes, or leaves. The number in each leaf is the mean of the response for the observations that fall there.

- In keeping with the tree analogy, the regions R1, R2, and R3 are known as terminal nodes
- Decision trees are typically drawn upside down, in the sense that the leaves are at the bottom of the tree.
- The points along the tree where the predictor space is split are referred to as internal nodes
- In the hitters tree, the two internal nodes are indicated by the text Years<4.5 and Hits<117.5.

- INTERPRET: Years most important.
    + Less experience = Hits made in the previous year play little role in salary
    + 5+ yrs of experience - Hits made does affect salary

</div>

## Ensemble Methods to improve accuracy 
<div class="column-left">
**Bootstrapping/Bagging**$^{1}$:

- Take repeated samples from the training data set
- Generate B different bootstrapped training data sets and grow a tree on each
    + Take the average of all predictions from each bootstrapped training set  (or for classification take the majority vote)
    + Can compute out-of-bag error estimate (i.e., like cross-validation)
</div>

<div class="column-left">

</div>

```{r, echo = FALSE, message=FALSE, warning= FALSE, error=FALSE,out.width = "450px", out.height='450px'}
knitr::include_graphics("bagging.png")
```
$^{2}$
<div class="notes">

- BAGGING CLASSIFICATION: we record the class predicted by each of the B trees and take a majority vote. The overall prediction is the most commonly occurring class among the B predictions
    + Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around two-thirds of the observations.
    + The remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations.
    + We can predict the response for the ith observation using each of the trees in which that observation was OOB. This will yield around B/3 predictions for the ith observation, which we average.

BAGGING SUMMARY: Recall that bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predictive model. Each tree is built on a boot-strap data set INDEPENDENT OF OTHER TREES 

**PICTURE**:
- Schematic of the RF algorithm based on the Bagging (Bootstrap 1Aggregating) method. (1) Stage 1: Use bootstrap method to sample M subsets from the original training data sets. (2) Stage 2: Build Independent decision trees for model training using input covariates (v). For each individual decision tree, the prediction con?dence (posterior probabilityp?Pdjv) increases from the root toward the leaves. (3) Stage 3: Obtain prediction from each bootstrap tree over Mreplications. (4) Stage 4: Decide the ?nal result by average or majorityvoting 

- COMMON QUESTIONS:
    + Building multiple trees- one tree does not result
    + Each observation has multiple predictions. For that given observations - we average over all predictions for that observation to get the final predict. Same in classification actually but it's an averaging of 1/s 0/s to get a probabolity

</div>


## Ensemble Methods to Improve Accuracy$^{1}$
<div class="column-left">
**Boosting**:

- Similar to bagging except that the trees are grown _sequentially_ using information from previously grown trees VS. Bagging where each tree is built on a bootstrapped df independent of other trees
- Each new tree is built to emphasize the observations that the previous trees mis-classified in order to improve on performance of the previous mix of trees
    + We do this by fitting small trees to the residuals to slowly improve $\hat{f}$ where it isn't performing well.
- Given sequential fitting and shrinkage parameters don't need to grow as large of trees.

</div>

<div class="column-left">

- Set $\hat{f}(x)=0$ and $r_{i} = y_{i}$ for all _i_ in the training set
- For _b_ = 1,2,..., _B_, repeat:
    + Fit a tree $\hat{f}^{b_{1}}$ with _d_ splits to the training data
    + Update $\hat{f}$ by adding in a shrunken version of the new tree 
\[
\hat{f}(x_{i}) \leftarrow \hat{f}(x_{i}) + \lambda\hat{f} ^{b_{1}}(x_{i}) 
\]
    + Update the residuals
\[
r_{i}\leftarrow r_{i}-\lambda\hat{f} ^{b_{1}}(x_{i})
\]
    + Output the boosted model
\[
\hat{f}(x) = \sum_{b=1}^{B}\lambda\hat{f} ^{b_{1}}(x)
\]
</div>

<div class="notes">

- Like bagging, boosting is a general approach that can be applied to many statistical learning methods for regression or classification. We only discuss boosting for decision trees.
- growing the tree to the residuals add it into the function, update the residuals, and continue. Trees are not independent of one another

-Given the current model we fit a decision tree to the residuals from the model. ADd this new tree to the fitted fnction to update the residuals. 
- Current model starts off at zero and add in some shrunken version of it to our model
- Update as the residuals change by a corresponding amount
-EACH TREE GROWN TO THE RESIDUALS LEFT OVER FROM THE PREVIOUS SET OF TREES 

- MENTION boosting for classification is similar but a bi tmore complited - won't cover here.  Basically it starts by initiating equal weights per sample, fit classficaiton tree using sample wight, for misclassified samples increase sample eight save a stagg weight based on performance of current model and redo 

COMMON QUESTIONS - CHECK 
  - Start off with a funciton for first tree = 0 
  - Residuals are the data. 
  - AFTER THE FIRST TREE MODELING THE RESIDUALS - trying to reduce the discrepancy between the residuals from the first and second. build a 
collection of trees captuing diff info complimenting one another 
  - ADD INDICES TO THE TREES AND THE FUNCTIONS FROM 1...B

</div>


## Ensemble Methods to improve accuracy$^{1}$ 

<div class="column-left">
**A number of tuning parameters for Boosting Models**: 

- Number of trees _B_ 
    + Way to control over fitting. 
    + Use cross-val to select _B_
- Shrinkage Parameter $\Lambda$
    + small positive number. 
    + Controls the rate at which boosting learns (i.e., .01, .001)
- Depth _d_ 
    + number of splits in each tree
    + Try a few values (1,2,4,8). 
</div>


<div class="column-left">

```{r, echo = FALSE, message=FALSE, warning= FALSE, error=FALSE,out.width = "450px", out.height='450px'}
knitr::include_graphics("gradientboosting.jpg")
```
$^{3}$
</div>

<div class="notes">
TUNING PARAMETERS:

- DEPTH: if d = 1, only 1 variable per tree and no interactions. If d = 2 , interaction
- SHRINKAGE: remember every time we grow a tree we don't accept the full tree rather we shrink it back by a fraction. (sort of like pruning)

</div>

## Random Forests and Boosting: "Leveraging"  Multiple Decision Trees
**Combine Regression Trees, Bagging or Boosting, and Some Tweaks to Improve Accuracy**


<div class="column-left">
_Random Forests_:

- Combine decision trees with bagging to achieve very high accuracy
- Similar to the Bagging with the addition of a small tweak that decorrelates trees
    + Each time a split is considered only _m_ predictors chosen as split candidate from the full set of predictors and the split is only allowed to use one of them.
    + Fresh selection taken at each split
    + _m_ can take on different values, typically $m\approx \sqrt{p}$
- Don't have to use pruning (i.e., fully grown trees)
    
</div>

<div class="column-left">
_Ensemble of Decision Trees + Boosting_:

- Basically what was described above. Based on _weak_ learners or shallow trees.

</div>


<div class="notes">

- as in bagging, we build a number of decision trees on bootstrapped training samples
- but each time a split is considered a random selection of m predictors is chosen as split candidates from the full set of p predictors. the split is allowed to use only one of those m predictors. A fresh selection of m predictors is taken at each split.
    + THIS LEADS TO REDUCED VARIANCE AFTER PREDICTION FROM TREES ARE AVG.
    + **m = p WOULD JUST BE BAGGING**
    + Also can think of m  as the number of predictors available for splitting at each interior tree node
  
</div>

## Sometimes simple is better$^{1}$ 

```{r, echo = FALSE, message=FALSE, warning= FALSE, error=FALSE,out.width = "800px", out.height='500px'}
knitr::include_graphics("simple_better.png")
```

## Interpreting the Black Box

<div class="column-left">
**For bagged/RF and boosting**: 

- Regression:
    * The total amount that the RSS is decreased due to splits over a given predictor, averaged over all trees. Large = Important
    
- Classification: 
    * The total amount a similar error measures (i.e., Gini index, entropy) is decreased by splits over a given predictor, averaged over all trees

</div>


<div class="column-left">

```{r message=FALSE, warning= FALSE, error=FALSE, echo=FALSE, out.width = "500px", out.height='500px'}
library(dplyr)
library(ggplot2)
load(file = "imp_df.Rdata")

varimpplot <- imp_df %>%  
  ggplot(aes(x = reorder(var, Overall),Overall)) + 
  geom_bar(stat = "density") +
  coord_flip() + 
  ylab('Variable Importance')+
  xlab('Variable')

varimpplot
```

</div>


## Favorite Resouces: Machine Learning Concepts 

<div class="column-left">

**Nuts and Bults** 

- "An Introduction to Statistical Learning (ISL)" by James, Witten, Hastie and Tibshirani
    + Available for free online 
    + The best, in my opinion, clear overview of several foundational ML concepts 
    + More approachable videos, code, and slides found [here](https://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/)

- Andrew Ng's Machine Learning Course on Coursera
    + Great online course to really understand the fundamentals
    + Free (unless you want "certificate") and readily available [here](https://www.coursera.org/learn/machine-learning)
    + Be prepared for some math 

</div>

<div class="column-left">

**Quicker Crash Courses**

- [Data Camp Caret Course](https://www.datacamp.com/)
    + Great for learning R in general, but specific course on the Caret Package
    + ~$30 monthly subscription (can sometimes access first portal for free)
- [Super High-level on-line Blog Post](https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/)
    +  Truly the bare minimum in terms of background, but covers a lot of ground 
    +  Enough to get started and good code examples in Python and R

</div>


## Favorite Resouces: Visualization and Understanding

[**xgboostExplainer**](https://medium.com/applied-data-science/new-r-package-the-xgboost-explainer-51dd7d1aa211):

- New package that makes your XGBoost model "as interpretable as a single decision tree"

[**LIME**](https://cran.r-project.org/web/packages/lime/index.html):

- Local Interpretable Model-Agnostic Explanations$^{4}$
- Nice tutorial [here](https://www.r-bloggers.com/explaining-complex-machine-learning-models-with-lime/)
- Broad capabilities _for classification_: can apply with any model for which you can obtain prediction probabilities

[**ICEbox**](https://github.com/kapelner/ICEbox)$^{5}$:

- Allow you to visualize the model estimated by any supervised learning algorithm
- Classical partial dependence plots (PDPs) help visualize the average partial relationship between the predicted response and one or more features, _particularly in the presence of substantial interaction effects_
- [Good overview of this and similar resources](https://www.r-bloggers.com/beyond-beta-relationships-between-partialplot-icebox-and-predcomps/)

##JUST GIVE ME A BUTTON

[h2o auto ML](https://cran.r-project.org/web/packages/h2o/h2o.pdf):

- Available in multiple language and for multiple platforms
- h2o has FAR MORE capabilities than this "h2o.automl" function and is definitely worth checking out
- automl function automates the supervised machine learning model training process fir a number of models
- [Nice tutorial here](http://www.business-science.io/business/2017/09/18/hr_employee_attrition.html) 
    + **Disclaimer** in my experience there are few data sets that come ready for a function like this without pre-processing. _Garbage in - garbage out_

##Shameless Plug

<div class="column-left">
[Women in Analytics Conference](https://www.womeninanalytics.org/):

- 3/15/18 in Columbus
- All day conference packed with technical and strategic talks from speakers all over the country
- Get your tickets now, apply for one of our remaining speaker spots, or our data viz competition
- Also check out the great ways to attend for free

</div>

<div class="column-left">

```{r, echo = FALSE, message=FALSE, warning= FALSE, error=FALSE,out.width = "450px", out.height='450px'}
knitr::include_graphics("WA_Logo_HiRes-01_redsized.png")
```

</div>

## Citations

$^{1}$ James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). _An introduction to statistical learning (Vol. 112)_. New York: springer.

$^{2}$ He, X., Chaney, N. W., Schleiss, M., & Sheffield, J. (2016). Spatial downscaling of precipitation using adaptable random forests. _Water Resources Research, 52(10)_, 8217-8237.

$^{3}$ [https://dimensionless.in/gradient-boosting/](https://dimensionless.in/gradient-boosting/)

$^{4}$ Ribeiro, M. T., Singh, S., & Guestrin, C. (2016, August). Why should i trust you?: Explaining the predictions of any classifier. _In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 1135-1144)_. ACM. Chicago	

$^{5}$ Goldstein, A., Kapelner, A., Bleich, J., & Pitkin, E. (2015). Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation. _Journal of Computational and Graphical Statistics, 24(1)_, 44-65. Chicago	



